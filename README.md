# Intro: Web-Scrapper Tool
This is a web scraping tool designed to extract data from webpages. It provides users with the flexibility to define any number and structure of schema as long as the associated attributes are present in the webpage's HTML elements.

# Working demo of Web Scraper Application
[Streamlit Demo Video Link](https://drive.google.com/drive/folders/1b4JbdxkGCsisUacBMhtx9u4HAaxptWtY?usp=sharing)

# Web Scrapper Live Application on Streamlit
[Deployed App Link](https://web-scrapper-noman.streamlit.app/)

# Instructions for running this project locally
## Step 1: Create a Directory for the Web Scraper Project

Assuming you're using WSL Ubuntu in VS Code and have Python 3 installed in your base environment, execute the following command in your terminal:

```bash
mkdir web-scraper
```
This command will create a directory named 'web-scraper' in your current location.

## Step 2: Navigate to the Web Scraper Directory

To access the 'web-scraper' directory, use the following command in your terminal:

```bash
cd web-scraper/
```
This command will change your current directory to 'web-scraper'.
## Step 3: Clone the Web Scraper Repository

Clone the Web Scraper repository from GitHub using the following command:

```bash
git clone https://github.com/noman024/web-scrapper.git
```
This command will create a local copy of the repository in your current directory.

## Step 4: List Contents of the Directory

To view the contents of the current directory, execute the following command:

```bash
ls
```
This command will list all the files and directories in the current directory.
## Step 5: Navigate to the Web Scraper Directory

Change your current directory to the 'web-scrapper' directory by running:

```bash
cd web-scrapper/
```
This command will move you into the 'web-scrapper' directory that is just installed from git.
## Step 6: List Contents of the Web Scraper Directory

To view the contents of the 'web-scrapper' directory, execute the following command:

```bash
ls
```
This command will list all the files and directories within the 'web-scrapper' directory that is just installed from git.

## Step 7: Install Required Dependencies

Install the dependencies required for the Web Scraper project by running:

```bash
pip install -r requirements.txt
```
This command will install all the packages listed in the 'requirements.txt' file.

## Step 8: Run the Web Scraper Application

Launch the Web Scraper application by executing the following command:

```bash
streamlit run app.py
```
This command will start the Streamlit server and run the Web Scraper application.

## Step 9: Access the Web Scraper Tool

Open your web browser and navigate to the following link:

[http://localhost:8501/](http://localhost:8501/)

This will direct you to the locally hosted instance of the Web Scraper tool, where you can start using the web scraping functionality.

## Feel Free to Contribute

If you find this Web Scraper tool useful, please consider forking the repository, giving it a star, or contributing in any other way you see fit. Your support is greatly appreciated!

Thank you for using the Web Scraper tool!
